{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22db8b42",
   "metadata": {},
   "source": [
    "# Thematic Analysis Notebook\n",
    "\n",
    "This notebook assigns sentiment scores and extracts actionable themes from app reviews. It expects the preprocessed CSV at `Data/processed/reviews_processed.csv`. Outputs are saved to `Data/processed/reviews_final.csv` and `Data/processed/theme_examples.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f1d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip installs if already available\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Install dependencies - run once in this environment\n",
    "# !pip install -q scikit-learn nltk spacy tqdm matplotlib seaborn wordcloud\n",
    "# !python -m spacy download en_core_web_sm -q\n",
    "print('Skip installs if already available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b73b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed reviews from: c:\\Users\\hp\\Desktop\\10  Academy\\week 2\\Assigniments\\Customer-Experience-Analytics-for-Fintech-Apps\\Data\\processed\\reviews_processed.csv\n",
      "Loaded 2,100 reviews\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to use project config for data paths when available\n",
    "try:\n",
    "    from Scripts.config import DATA_PATHS\n",
    "    candidate = DATA_PATHS.get('processed_reviews')\n",
    "except Exception:\n",
    "    candidate = None\n",
    "\n",
    "# Candidate paths to search (absolute-resolved later)\n",
    "candidates = [candidate, 'Data/processed/reviews_processed.csv', 'data/processed/reviews_processed.csv']\n",
    "searched = []\n",
    "src = None\n",
    "# Try multiple base directories so notebook works when running from `notebooks/` or repo root\n",
    "bases = [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent, Path.cwd().parent.parent.parent]\n",
    "for c in candidates:\n",
    "    if not c:\n",
    "        continue\n",
    "    p = Path(c)\n",
    "    # If absolute, check directly\n",
    "    if p.is_absolute():\n",
    "        searched.append(str(p))\n",
    "        if p.exists():\n",
    "            src = p\n",
    "            break\n",
    "    else:\n",
    "        # Try resolving relative to several likely base directories\n",
    "        for base in bases:\n",
    "            candp = base / p\n",
    "            searched.append(str(candp))\n",
    "            if candp.exists():\n",
    "                src = candp\n",
    "                break\n",
    "        if src:\n",
    "            break\n",
    "# As a final guard, look for Data/processed under upward parents of cwd\n",
    "if src is None:\n",
    "    for base in bases:\n",
    "        repo_try = base / 'Data' / 'processed' / 'reviews_processed.csv'\n",
    "        searched.append(str(repo_try))\n",
    "        if repo_try.exists():\n",
    "            src = repo_try\n",
    "            break\n",
    "if src is None:\n",
    "    raise FileNotFoundError('Could not find processed reviews CSV. Searched:\\n' + '\\n'.join(searched))\n",
    "\n",
    "print('Loading processed reviews from:', src)\n",
    "df = pd.read_csv(src)\n",
    "print(f'Loaded {len(df):,} reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825c13a",
   "metadata": {},
   "source": [
    "## Sentiment analysis (VADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "777733d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_label\n",
       "positive    1231\n",
       "neutral      591\n",
       "negative     278\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure resources\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_score(text):\n",
    "    try:\n",
    "        return sia.polarity_scores(str(text))['compound']\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "df['sentiment_score'] = df['review_text'].apply(vader_score)\n",
    "df['sentiment_label'] = df['sentiment_score'].apply(lambda s: 'positive' if s>=0.05 else ('negative' if s<=-0.05 else 'neutral'))\n",
    "df['sentiment_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b84d8",
   "metadata": {},
   "source": [
    "## Preprocessing and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6826bc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üôèüëç</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very Good</td>\n",
       "      <td>very good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goof</td>\n",
       "      <td>goof</td>\n",
       "      <td>goof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good!</td>\n",
       "      <td>good!</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good jop</td>\n",
       "      <td>good jop</td>\n",
       "      <td>good jop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_text clean_text lemmatized\n",
       "0          üôèüëç                      \n",
       "1   Very Good  very good       good\n",
       "2        goof       goof       goof\n",
       "3       good!      good!       good\n",
       "4    good jop   good jop   good jop"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean text function\n",
    "def clean_text(t):\n",
    "    if pd.isna(t):\n",
    "        return ''\n",
    "    s = str(t)\n",
    "    s = re.sub(r'http\\S+|www\\.\\S+', ' ', s)\n",
    "    s = re.sub(r'[^\\x00-\\x7F]+', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip().lower()\n",
    "    return s\n",
    "\n",
    "df['clean_text'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "# Try spaCy lemmatization if available\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    def lemmatize_texts(texts):\n",
    "        docs = list(nlp.pipe(texts, batch_size=64))\n",
    "        out = []\n",
    "        for doc in docs:\n",
    "            tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "            out.append(' '.join(tokens))\n",
    "        return out\n",
    "except Exception:\n",
    "    from nltk.corpus import stopwords\n",
    "    sw = set(stopwords.words('english'))\n",
    "    def lemmatize_texts(texts):\n",
    "        out = []\n",
    "        for s in texts:\n",
    "            tokens = re.findall(r'\\b[a-z]{2,}\\b', s)\n",
    "            tokens = [w for w in tokens if w not in sw]\n",
    "            out.append(' '.join(tokens))\n",
    "        return out\n",
    "\n",
    "df['lemmatized'] = lemmatize_texts(df['clean_text'].fillna('').tolist())\n",
    "df[['review_text','clean_text','lemmatized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d36fa1",
   "metadata": {},
   "source": [
    "## TF-IDF keyword extraction per bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecf4b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank: Bank of Abyssinia ‚Üí good, app, work, bad, nice, bank, boa, well, good app, mobile, banking, bad app\n",
      "Bank: Commercial Bank of Ethiopia ‚Üí good, app, good app, nice, well, excellent, cbe, ok, bank, update, like, use\n",
      "Bank: Dashen Bank ‚Üí good, app, nice, bank, dashen, super, wow, fast, good app, easy, work, banking\n"
     ]
    }
   ],
   "source": [
    "def top_tfidf(series, n=20, ngram_range=(1,2)):\n",
    "    try:\n",
    "        vec = TfidfVectorizer(max_features=500, ngram_range=ngram_range, token_pattern=r'\\b[a-z]{2,}\\b')\n",
    "        X = vec.fit_transform(series.fillna(''))\n",
    "        sums = X.sum(axis=0).A1\n",
    "        terms = vec.get_feature_names_out()\n",
    "        top_idx = sums.argsort()[::-1][:n]\n",
    "        return [terms[i] for i in top_idx]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "bank_keywords = {}\n",
    "for bank, sub in df.groupby('bank_name'):\n",
    "    kws = top_tfidf(sub['lemmatized'], n=25)\n",
    "    bank_keywords[bank] = kws\n",
    "    print(f'Bank: {bank} ‚Üí ' + ', '.join(kws[:12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a3ce3",
   "metadata": {},
   "source": [
    "## Rule-based theme assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a0f14a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top themes (counts): [('Other', 1677), ('Transactions & Payments', 156), ('Performance & Reliability', 129), ('Customer Support', 109), ('User Interface & Experience', 98), ('Account Access Issues', 77)]\n"
     ]
    }
   ],
   "source": [
    "THEME_KEYWORDS = {\n",
    "    'Account Access Issues': ['login','otp','password','pin','sign in','blocked','access'],\n",
    "    'Performance & Reliability': ['slow','lag','crash','error','not working','freeze','hang','failed'],\n",
    "    'User Interface & Experience': ['ui','user friendly','navigation','design','confusing','layout'],\n",
    "    'Transactions & Payments': ['transfer','payment','deposit','withdraw','transaction','balance'],\n",
    "    'Customer Support': ['support','service','response','contact','ignored','help']\n",
    "}\n",
    "\n",
    "def assign_theme(text):\n",
    "    t = str(text).lower()\n",
    "    hits = []\n",
    "    for theme, kws in THEME_KEYWORDS.items():\n",
    "        for kw in kws:\n",
    "            if kw in t:\n",
    "                hits.append(theme)\n",
    "                break\n",
    "    if not hits:\n",
    "        return ['Other']\n",
    "    return list(dict.fromkeys(hits))\n",
    "\n",
    "df['identified_themes'] = df['lemmatized'].apply(assign_theme)\n",
    "from collections import Counter\n",
    "c = Counter([t for row in df['identified_themes'] for t in row])\n",
    "print('Top themes (counts):', c.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b81d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final CSV to: Data\\processed\\reviews_final.csv\n",
      "Saved theme examples to Data/processed/theme_examples.json\n",
      "Saved theme examples to Data/processed/theme_examples.json\n"
     ]
    }
   ],
   "source": [
    "# Save final file and theme examples\n",
    "out = Path('Data/processed/reviews_final.csv')\n",
    "out.parent.mkdir(parents=True, exist_ok=True)\n",
    "cols = ['review_id','review_text','rating','review_date','bank_name','sentiment_label','sentiment_score','identified_themes']\n",
    "present = [c for c in cols if c in df.columns]\n",
    "df.to_csv(out, columns=present, index=False)\n",
    "print(f'Saved final CSV to: {out}')\n",
    "# theme examples\n",
    "theme_examples = {}\n",
    "for theme in set([t for row in df['identified_themes'] for t in row]):\n",
    "    theme_examples[theme] = []\n",
    "for _, r in df.iterrows():\n",
    "    for t in r['identified_themes']:\n",
    "        if len(theme_examples[t]) < 5:\n",
    "            theme_examples[t].append({'review_id': r.get('review_id'), 'text': r.get('review_text')})\n",
    "with open('Data/processed/theme_examples.json','w',encoding='utf-8') as fh:\n",
    "    json.dump(theme_examples, fh, ensure_ascii=False, indent=2)\n",
    "print('Saved theme examples to Data/processed/theme_examples.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5183627",
   "metadata": {},
   "source": [
    "---\n",
    "Notes:\n",
    "- This notebook expects `Data/processed/reviews_processed.csv` (matching repo).\n",
    "- If you prefer `data/` lowercase, change `candidate` or rename the folder.\n",
    "- To improve themes, update `THEME_KEYWORDS` or add a clustering step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
